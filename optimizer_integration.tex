\documentclass[10pt]{article}
\title{Optimizer Integration}

\newcommand{\vgcomment}[1]{\noindent {\bf \\VG: #1 \\}}
%\newcommand{\vgcomment}[1]{}

\begin{document}
\maketitle

We model the expected response time for the algorithms in terms of their time complexity and additionally the writes incurred by each one of them during the course of their execution. 
\section{Sort}
\subsection{Quicksort}
In case of native quicksort, the average case time-complexity is $N_R log_2 (N_R)$. Additionally, as calculated in earlier section, the writes is of the order of $N_R L_R (0.5 log_2\frac{N_R L_R}{D} + 1)$. This would incur additional time of $\lambda \times N_R L_R (0.5 log_2\frac{N_R L_R}{D} + 1)$. Thus, the total time complexity is $N_R (log_2 (N_R) + \lambda L_R (0.5 log_2(\frac{N_R L_R}{D}) + 1))$.


\subsubsection{Level-wise latency} 
There are $log_2 N_R$ levels. The data will be fetched from PCM till level k in the binary tree for quicksort where $\frac{N_R L_R}{2^k} > D$ i.e $k < log_2 \frac{N_R L_R}{D}$. Similarly, DRAM latency is between levels $\frac{N_R L_R}{2^k} < D$ to $\frac{N_R L_R}{2^k} > L2$ giving k between $ log_2 \frac{N_R L_R}{L2} -  log_2 \frac{N_R L_R}{D}$ . Identical calculation holds for other levels. 


Also, since the latency is per line, we have to convert the data in terms of the lines also.

Hence, the total read latency will be - $N_R L_R (\frac{t_{pcmread}}{line_{PCM}} (log_2 \frac{N_R L_R}{D}) + \frac{t_{dram}}{line_{DRAM}} (log_2 \frac{N_R L_R}{L2} - log_2\frac{N_R L_R}{size_{D}}) + \frac{t_{L2}}{line_{L2}} (log_2 \frac{N_R L_R}{size_{L1}} - log_2\frac{N_R L_R }{size_{L2}}) + \frac{t_{L1}}{line_{L1}} (log_2 N_R - log_2\frac{N_R L_R }{size_{L1}}))  $. This of course assumes $log_2 N_R > log_2\frac{N_R L_R }{size_{L1}}$ i.e $size_{L1} > L_R$
\\

The above expression simplies to
\begin{equation}
N_R L_R log_2(\frac{(N_R L_R)^{\frac{ t_{pcmread} }{line_{pcm}}}}{D^{\frac{ t_{pcmread} }{line_{pcm}} - \frac{t_{DRAM}}{line_{DRAM}}} \times L_2^{\frac{t_{DRAM}}{line_{DRAM}} - \frac{t_{L2}} {line_{L2}}  } \times L_1^{ \frac{t_{L2}} {line_{L2}} - \frac{ t_{L1} } {line_{L1}}}  \times  L_R^{\frac{ t_{L1} } {line_{L1}}} } )
\end{equation}



Note that there is no $L_R$ in the last term of the expression since as mentioned before, the number of levels are $log_2 L_R$.

\subsection{Multi-pivot quicksort}
Coming to Multi-pivot quicksort, the time-complexity is again given by $N_R log_2 (N_R)$. Similarly, the writes are given by $2 N_R L_R$ if $N_R L_R > D $, otherwise $N_R L_R$. The total time complexity in this case is $N_R (log_2 (N_R) + 2\lambda L_R)$

For level-wise read latency for Multi-pivot sort, for count pass, the data would be always retrieved from PCM. For swap pass - it may or may not be depending on the relative sizes, thus incurring $min(2, \lceil \frac{N_R L_R}{D} \rceil) \times \frac {N_R L_R}{line_{pcm}} \times t_{pcm}$. 

For each of the $2N_R$ elements, we fetch $log_2 p$ pivots due to binary search among p pivots. Assuming each pivot starts from a separate PCM line, $\frac{L_R}{line_{Li}}\rceil \times t_{Li} $ time will be incurred for fetching each pivot. Hence, we pay a total additional latency of 
\begin{equation}
2  N_R  \times log_2 p \times \lceil \frac{L_R}{line_{Li}}\rceil \times t_{Li}
\end{equation}
where Li is the level of cache where all p pivots can fit.

Afterwards, assuming we get ideal pivots such that all the partitions fit are D sized, the simple quicksort on these partitions will incur writes which are given by the earlier equation replacing $N_R L_R$ by $D$ giving latency as  
\begin{equation}
D log_2(\frac{D^\frac{t_{DRAM}}{line_{DRAM}}}{L_2^{\frac{t_{DRAM}}{line_{DRAM}} - \frac{t_{L2}} {line_{L2}}} \times L_1^{\frac{t_{L2}} {line_{L2}} - \frac{ t_{L1} } {line_{L1}}}     \times  L_R^{\frac{ t_{L1} } {line_{L1}}} } )
\end{equation}
\vgcomment {Check in all expressions where ceil and floor functions apply, especially the number of cache lines should be ceil``ed" before multiplying with latency per line}

\section{Join}
\subsection{Build Phase}
Assume that the entire hash table can fit in some cache level $L_i$ (this might even be DRAM for that matter). We assume that the access to buckets is uniformly random, i.e., each access causes last level cache miss. Also, for each bucket's chained list, we assume that the new entry is always inserted at the beginning of the chain itself instead of hopping all the way to the end which anyway doesn't make sense.

The build phase involves jumping to the corresponding bucket to which the inner table tuple hashes - which will incur 1 cache line miss. This is common to both. 
\subsubsection{Conventional HJ}
For each of the $N_R$ tuples, apart from fetching \emph{1 line} for the right bucket, additional $\lceil \frac {size_{entry} + P}{size_{Li}} \rceil$ lines will be fetched for making an entry. This gives a total latency of 
\begin{equation}
t_{Li} \times N_R\times (\lceil \frac {size_{entry} + P}{size_{Li}} \rceil + 1)
\end{equation}

\subsubsection{Optimized HJ}
As in the previous case, the calculation is similar except: \\
a)there is no separate pointer to consider apart from the $size_{entry}$\\
b) that there is an additional line miss for accessing the page bitmap since a page can be very large in size; which would lead to the entry and the bitmap being far apart. Thus the corresponding equation is : 
\begin{equation}
t_{Li} \times N_R\times (\lceil \frac {size_{entry}}{size_{Li}} \rceil + 1 + 1(Bitmap))
\end{equation}



\subsection{Probe Phase}

\vgcomment{
a) Need to consider the fact that false positives occur due to small hash value. Where is this being handled?

b) See if our opt for some of the algos always dominates the conventional in both metrics regardless of parameter values. Then there is no trade-off happening!}

\subsubsection{Conventional HJ}
For search of each of $N_S$ tuples, we only jump once to the bucket. However, each entry in Conv. HJ begins on a separate line since the entries aren't contiguous. Thus the delay is 
\begin{equation}
t_{Li} \times N_S\times (1(bktJump) + (\frac{N_R}{B} \times \lceil \frac {size_{entry} + P}{size_{Li}} \rceil))
\end{equation}
The bucket jump term can be ignored giving us 
\begin{equation}
t_{Li} \times N_S\times \frac{N_R}{B} \times \lceil \frac {size_{entry} + P}{size_{Li}} \rceil
\end{equation}
\nonumber

\subsubsection{Opimized HJ}
Here, the main difference from previous is that all the tuples are contiguous, hence the ceiling function comes after multiplying $size_{entry}$ with $N_R/B$. We ignore the additional bitmap lookup term since it is very small.
\begin{equation}
t_{Li} \times N_S\times \lceil\frac{N_R}{B} \times  \frac {size_{entry}}{size_{Li}} \rceil
\end{equation}

\section{Challenges in Optimizer Integration}
\begin{enumerate}
\item The estimated writes and cycles for \emph{each} operator has to be put in place, not just the optimized operators.

a) Shall we restrict ourselves only to plans containing the optimized operators? If yes, how to do that?\\
b) Making this automated will be a huge overhead because:\\
- We would need exact (not estimated) inner and outer tuples at each level of the plan tree. To make this work, selectivity injection or some such mechanism has to be resorted to.\\
- there is no longer any cost associated with fetching tuples from the disk\\
- the actual structures allocated by Postgres are far complicated to estimate \emph{writes per tuple} due to each structure. The best thing instead would be to get optimal plan from there using our own code values. Thereafter, execute my own code so that the estimates match.


\item If we cannot use present estimates of cycles for each operator, but have to come up with a slightly more exact figure rather than order notation, it is a whole new work. Instead, just focus on adding additional cycles due to writes for each estimate? For this, an absolute value has to be put in place instead of order notation which needs to be looked at.

\item How to give the Pareto set is another challenge for which EXPAND paper should act as a reference. Answer: We will use a $\lambda \%$ leeway in cycles and come up with the least writes plan in that window.


\end{enumerate}

\section{Understanding Optimizer Code}

\begin{enumerate}
\item add\_path simply adds another path to the reloptinfo of the concerned relation that it is dealing with (called parent-relation in code) in the series of its pathlist. Of-course there is also pruning right then and there to discard useless paths

\item the tree is built bottom-up like DP. If it is a join node for instance, function add\_paths\_to\_joinrel will be called, which will enumerate all possible join methods and each of those methods will internally call add\_path for the \emph{same} reloptinfo node creating a list of possible paths.

\item the create\_*\_path functions do the actual spade-work of making the path and costing it via cost\_*  functions. It is within these functions that the new path node is created and the previous left and right paths are made the children of this new node. Example create\_hashjoin\_path which internally calls cost\_hashjoin.

\item my changes would feature in 3 places - add\_paths\_to\_joinrel and similar per operator functions to now feature in other types of joins, add\_path function itself to change pruning mechanism and also in the cost\_* functions. Also, a new metric needs to be added in the reloptinfo called "writes". It is to be decided whether we should do local pruning or an "at-once" final global pruning at the root node of the full plan tree 

\item make\_agg function in createplan.c is responsible for inserting agg\_nodes. Its callee is a function called grouping\_planner. Check it out.

\item match\_unsorted\_outer corresponds to both merge join and NL join.

\item A useful excerpt from README for joins - what is meant by inner and outer relations :

\begin{verbatim}
Joins always occur using two RelOptInfos.  One is outer, the other inner.
Outers drive lookups of values in the inner.  In a nested loop, lookups of
values in the inner occur by scanning the inner path once per outer tuple
to find each matching inner row.  In a mergejoin, inner and outer rows are
ordered, and are accessed in order, so only one scan is required to perform
the entire join: both inner and outer paths are scanned in-sync.  (There's
not a lot of difference between inner and outer in a mergejoin...)  In a
hashjoin, the inner is scanned first and all its rows are entered in a
hashtable, then the outer is scanned and for each row we lookup the join
key in the hashtable.
\end{verbatim}
\item NL join takes almost each of the available paths in the child nodes of the outer. 

\begin{verbatim}
/*
			 * Always consider a nestloop join with this
			 * (i.e each one in the outer list) outer and
			 * cheapest-total-cost inner.  When appropriate, also consider
			 * using the materialized form of the cheapest inner, the
			 * cheapest-startup-cost inner path, and the cheapest innerjoin
			 * indexpaths.
			 */
\end{verbatim}
Hash Join has the following philosophy
\begin{verbatim}
/*
		 * We consider both the cheapest-total-cost and cheapest-startup-cost
		 * outer paths.  There's no need to consider any but the
		 * cheapest-total-cost inner path, however.
		 */
\end{verbatim}





\end{enumerate}
\end{document}